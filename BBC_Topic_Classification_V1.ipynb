{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a096bba5-6e37-4666-8eca-a22f2c57185c",
   "metadata": {},
   "source": [
    "# Supervised Topic Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d484d-4324-4098-9979-7d75aa870ad7",
   "metadata": {},
   "source": [
    "## Version 1.0\n",
    "(see **warning** below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db121b39-2094-4d17-a730-d87d9cb253da",
   "metadata": {},
   "source": [
    "Supervised Topic classification using the BBC News Kaggle dataset.\n",
    "\n",
    "See data source here https://www.kaggle.com/pariza/bbc-news-summary \n",
    "\n",
    "Last accessed on 2022-Mar-12.\n",
    "\n",
    "**Warning** \n",
    "This is a notebook created for my own learning. So it will be rough, and likely full of mistakes. I will try to keep some of the more interesting mistakes, or poor judgement, or bad choices, in so they can be used as learning points for myself and others. So over time I might split this notebook out into different versions just so there is not too much scrolling to the most up to date version \"correct\" code implementation. At the end of each version, I'll create a \"To do!\" to help bookmark the end of the current version and describe how I'll try and improve it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c140f670-1554-489e-a19b-826c1c073f61",
   "metadata": {},
   "source": [
    "### What is this Notebook?\n",
    "\n",
    "This Notebook has been created as a personal learning exercise.\n",
    "\n",
    "It focuses on using basic NLP tools to carry out topic \n",
    "classifcation (and maybe topic modelling)\n",
    "with a labelled dataset of BBC news articles. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7c703a-583d-4a06-9b98-1ac3dea444d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61878af7-9335-40fa-80d0-6b53e8245c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Raw data is .gitignore 'd. \n",
    "It is available from the source at the beginning of this notebook.\n",
    "\"\"\"\n",
    "import glob\n",
    "files = glob.glob('data/News Articles/**/*.txt',\n",
    "                  recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835ad34-a3fa-4515-8f9c-c60b7de4e52f",
   "metadata": {},
   "source": [
    "### The Data\n",
    "Each file is a .txt.\n",
    "\n",
    "We can generate our labelled column for the dataset by using the name of the \n",
    "parent folder of each .txt file.\n",
    "\n",
    "Then we'll build up a pandas DataFrame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1433aae2-c112-46c5-9b0c-7d877fc1de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,fileNames = zip(*[s.split('/')[2:] for s in files])\n",
    "corpus = pd.DataFrame(data = {\n",
    "    \"filePath\":files, \"label\":labels, \"fileName\":fileNames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b65c93-75c4-4100-a19b-bf28790a522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read each file, save to list, and append to the\n",
    "# corpus pd.DataFrame\n",
    "text = []\n",
    "for name in files:\n",
    "    with open(name,\"r\",encoding=\"unicode_escape\") as f:\n",
    "        text.append(f.read())\n",
    "corpus['text'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae4436-6fe8-48dc-bc44-a72b3202f91f",
   "metadata": {},
   "source": [
    "### Unicode Nightmares\n",
    "\n",
    "Regarding the \"unicode_escape\" keyword...\n",
    "There seem to be some mixed unicode standards in the raw BBC \n",
    "news articles. I've been a bit lazy and just used this to\n",
    "avoid errors.\n",
    "\n",
    "However this might come back to bite me when I tokenize \n",
    "my text and see dodgy looking tokens...\n",
    "\n",
    "There is the question, did I need to bother with this? \n",
    "Would sklearn have handled it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2576e-f568-4bc8-86bd-4fc1368a4d45",
   "metadata": {},
   "source": [
    "## Category (label) to numbers\n",
    "\n",
    "While our labelled dataset category is nice, most ML processes prefers numbers. There are a few ways we could create a number equivalent value for the `corpus['label']` column. Remember, this is just the label column we used for our supervised model. We're not editing a feature.\n",
    "\n",
    "{Do I even really need to do this?}\n",
    "\n",
    "As a side, if we were really playoung around and turning a categorical feature into something more ML-friendly, then we might want to consider using a **OneHotEncoder**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa67ff2-f429-4689-b2b4-dc9778e9b3ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrdinalEncoder\n\u001b[1;32m      2\u001b[0m ordinal_encoder \u001b[38;5;241m=\u001b[39m OrdinalEncoder()\n\u001b[0;32m----> 4\u001b[0m corpus_label_encoded \u001b[38;5;241m=\u001b[39m ordinal_encoder\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mcorpus\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m      5\u001b[0m corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m corpus_label_encoded\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "corpus_label_encoded = ordinal_encoder.fit_transform(corpus[['label']])\n",
    "corpus['cat'] = corpus_label_encoded\n",
    "# I've now learnt that what I'm doing here is MULIT-LABEL SUPERVISED CLASSIFICATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf02985-6c05-484d-b4f4-6facb3592963",
   "metadata": {},
   "source": [
    "## TF-IDF & Test-Train-Split\n",
    "\n",
    "In this example, I am going to be pretty fast and loose (a.k.a. not very scientific) and perform the TF-IDF BEFORE the test-train split. This is, I believe, generally ill advised as it will generate data leakage between the training and test sets. This bad practice also makes your model less generalizable to real-world examples where new text will have tokens/terms not in your original training data. \n",
    "\n",
    "See step three https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50\n",
    "\n",
    "**So why am I doing it here?**\n",
    "\n",
    "I want to create a first pass model, even if it does have some flaws, and then I will correct this later down the line. So, just bare with me. This is a learning journey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a2d033-7b82-46e9-9ab9-4dff0a83c012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0001', ..., 'zutons', 'zvonareva', 'zvyagintsev'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', ngram_range=(1,1))\n",
    "corpus_tfidf = vectorizer.fit_transform(corpus['text'])\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e6299-156f-4a9d-9a55-acef43d4fd24",
   "metadata": {},
   "source": [
    "### A few things to note...\n",
    "\n",
    "Removal of **stop words** is probably fine for this program, and will likely have a new positive to the model. (I should try this out!). But, in more advanced models it is generally not carried out as: \n",
    "\n",
    "* The definition of stop words is different between Python libraries and can even change over time within the same library. \n",
    "* You do lose information. Though as we're doing the tf-idf anyway, it is relatively minor. \n",
    "* tf-idf already balances out the frequent words. So why bother?\n",
    "\n",
    "I've also selected a 1-token **ngram** range. (I should also encode some testing for higher ranges). I'd recommend going ahead and googling ngrams if you are unaware of how to understand them. In this use case, I would likely benefit from having 2-grams as well as 1-grams, but it does blow up the calculation quite quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2e108-59dc-47ff-9a22-93898cd1aaef",
   "metadata": {},
   "source": [
    "## Test-Train-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847d4e2-40de-4d1f-8635-41af07bf6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets check out the data, making sure there's nothing dodgy going on \n",
    "# between our labelled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4009f8c2-8b31-415c-8cdf-79146964826f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede74f2b-a766-4ff0-8cd5-e0a0c5d61ca7",
   "metadata": {},
   "source": [
    "The contents of each category are a bit different. \n",
    "\n",
    "For the sake of showing\n",
    "the process, I will make sure our test-train-split pulls a proportional \n",
    "selection from each category. \n",
    "Aka **stratified sampling**.\n",
    "\n",
    "We've also not worried too much about assigning a unique identifier to each row. If we were perhaps planning to make this a production-level model where new data is added over time, we would need to fix this. New model runs with new data would want to avoid mixing the old test and the old train data. I.e. you'd want to ensure your future testing data contains only the old test data + a proportion of the new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb4676d-84e0-463f-bf66-154c96fdbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(corpus, corpus['label']):\n",
    "    strat_train_set = corpus.loc[train_index]\n",
    "    strat_test_set = corpus.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233eefa-73b3-4df9-a2c4-a1aa11e01fb4",
   "metadata": {},
   "source": [
    "Can demonstrate this generates (almost) the same proportions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64529373-8ef8-43e3-b6e8-9a6d1247a780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            0.229775\n",
       "business         0.229213\n",
       "politics         0.187079\n",
       "tech             0.180337\n",
       "entertainment    0.173596\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set['label'].value_counts()/len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd50fdaa-6432-4568-a5f4-ce83f2222e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            0.229663\n",
       "business         0.229213\n",
       "politics         0.187416\n",
       "tech             0.180225\n",
       "entertainment    0.173483\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['label'].value_counts()/len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c54d0-a756-4ca7-b2ed-62a13cb51f48",
   "metadata": {},
   "source": [
    "## Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d703ee0-ccc3-407c-a01f-40c30e38502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=4)\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea807daf-b287-430f-afcf-42bdd2a164ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=lda, X= corpus_tfidf.toarray()[train_index],y= strat_train_set['cat'],\n",
    "                         cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98332e49-4983-438b-b0cb-00dd13f36b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52808989, 0.3988764 , 0.53932584, 0.56741573, 0.51123596,\n",
       "       0.38764045, 0.47752809, 0.46629213, 0.58426966, 0.58426966])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb84590-32be-4b33-88f7-11d82d141aff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Result Discussion\n",
    "I'm being a bit impatient, because I haven't tested the Test data set yet on the model, but I can say that the scores above aren't great.\n",
    "\n",
    "First, these scores *might* be  normal for a first pass supervised LDA model. The dimensional reduction provided by LDA requires human-labels which may not be distinct enough, or there may just not be enough data to accurately capture the 5 categories.\n",
    "\n",
    "Other Stuff\n",
    "* Item 1\n",
    "* Item 2\n",
    "* Item 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2de5a-c868-47e4-adca-a7a743e8f62f",
   "metadata": {},
   "source": [
    "## Future Version Improvements\n",
    "\n",
    "* Correctly the highly questionable processing step of TF-IDF\n",
    "    * Turn into a sklearn pipeline\n",
    "    * Apply to test and train data seperately (fit_transform vs fit)\n",
    "* Change the validation method to more standard classifer-validation techniques, e.g.,:\n",
    "    * Confusion Matrix\n",
    "    * precision - recall plots (PR)\n",
    "    * POC plots\n",
    "* Play around with classifier options such as:\n",
    "    * One vs One Classifying\n",
    "    * One vs All Classifying\n",
    "* Encode a grid and randomized search to allow us to explore:\n",
    "    * effects of n-grams\n",
    "    * effects of stop word removal\n",
    "* Explore some explainable AI methods to clarify how models work\n",
    "    * Create textplots showing how different words sway the classification\n",
    "* Explore how we can use UNSUPERVISED models to seed into a supervised classification model\n",
    "\n",
    "* Blue Sky...\n",
    "    * Get more labelled news data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cebfc-e6dd-420c-9584-20dc2ba0b4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
