{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b5bc59-2d54-4124-b6d5-1f200d243868",
   "metadata": {},
   "source": [
    "# Supervised Topic Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643f623-96e4-45ab-8cdd-24ae06f35a38",
   "metadata": {},
   "source": [
    "## Version 2.0\n",
    "\n",
    "Welcome to my Supervised Toppic Classification Learning Experiment, using the BBC News Kaggle dataset. This series of Notebooks is to serve myself, and potentially others, as I learn bits of NLP relating to topic classification. Each notebook is split into versions to capture my design and learning progress, and to demonstrate in situ the mistakes and poor judgement calls I make. \n",
    "\n",
    "As the end of each iteration, I will leave some notes to discuss how I will (hopefully) improve my work in future versions. \n",
    "\n",
    "See data source here https://www.kaggle.com/pariza/bbc-news-summary \n",
    "\n",
    "Last accessed on 2022-Mar-12.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d815c9ea-865c-4090-95e6-83227ce9e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some standard dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe09a7d-ede8-4e23-b596-7a3964049023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're cloning the gitlab repo, this cell wont work for you unless you save the data until a similar directory.\n",
    "# Data is available in Notebook header text. \n",
    "\n",
    "import glob\n",
    "files = glob.glob('data/News Articles/**/*.txt',\n",
    "                  recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc4317-7360-404e-8835-d2ccd2c1b8b1",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The data repo contains **five** folders, {business, entertainment, politics, sport, tech} each containing a `.txt` file corresponding to one Article. \n",
    "\n",
    "I know already, because I looked at *some* of them, that these files are pretty clean. There is no header/footer text to remove or other structural features, but there are a lot of symbols for currency (£, $, Eur), percentages (%) etc. So these will need to be handled. \n",
    "\n",
    "**Things to consider**\n",
    "* The variation in article length\n",
    "* The number of articles per 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b0bc62-0475-4894-b90e-7a91c7f1b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create data 'labels' by using the name of the folder the Article is saved in\n",
    "labels,fileNames = zip(*[s.split('/')[2:] for s in files])\n",
    "# Then make a pandas.DataFrame to store the data\n",
    "corpus = pd.DataFrame(data = {\n",
    "    \"filePath\":files, \"topic\":labels, \"fileName\":fileNames})\n",
    "\n",
    "# Open and read each file, save to list, and append to the corpus pd.DataFrame\n",
    "# \"unicode_escape\" files with the symbols in the text\n",
    "text = []\n",
    "for name in files:\n",
    "    with open(name,\"r\",encoding=\"unicode_escape\") as f:\n",
    "        text.append(f.read())\n",
    "corpus['text'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693110b8-6885-45b0-a1bf-e4be1aa77feb",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "This task is strictly multi-Class classification. See https://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "We want our articles to have one label (target). They are either business, or sport etc. but cannot be both. Having multiple possible labels would be a multi-LABEL classification problem. \n",
    "\n",
    "The type of problem determines how we need to encode our labels. \n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "\n",
    "and https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "We will be doing Label Encoding, as we want to turn our string labels into numerical values which go from 0 to 1-n, where n = number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de00da7-747b-4f98-85c9-be15f65d1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Encoded labels\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(corpus['topic'].unique())\n",
    "corpus['topicLabel'] = le.transform(corpus['topic'])\n",
    "# Is this overkill? Maybe. It it makes me happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382e4a30-2cbf-4960-ad0a-c29965abf87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>topic</th>\n",
       "      <th>fileName</th>\n",
       "      <th>text</th>\n",
       "      <th>topicLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/News Articles/business/052.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>052.txt</td>\n",
       "      <td>Italy to get economic action plan\\n\\nItalian P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/News Articles/business/019.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>019.txt</td>\n",
       "      <td>India widens access to telecoms\\n\\nIndia has r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/News Articles/business/260.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>260.txt</td>\n",
       "      <td>Asia shares defy post-quake gloom\\n\\nIndonesia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/News Articles/business/007.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>007.txt</td>\n",
       "      <td>Jobs growth still slow in the US\\n\\nThe US cre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/News Articles/business/042.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>042.txt</td>\n",
       "      <td>UK Coal plunges into deeper loss\\n\\nShares in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>data/News Articles/entertainment/335.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>335.txt</td>\n",
       "      <td>De Niro film leads US box office\\n\\nFilm star ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>data/News Articles/entertainment/006.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>006.txt</td>\n",
       "      <td>Bennett play takes theatre prizes\\n\\nThe Histo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>data/News Articles/entertainment/187.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>187.txt</td>\n",
       "      <td>Double eviction from Big Brother\\n\\nModel Capr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>data/News Articles/entertainment/034.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>034.txt</td>\n",
       "      <td>Vera Drake scoops film award\\n\\nOscar hopefuls...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>data/News Articles/entertainment/228.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>228.txt</td>\n",
       "      <td>Joy Division story to become film\\n\\nThe life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      filePath          topic fileName  \\\n",
       "0          data/News Articles/business/052.txt       business  052.txt   \n",
       "1          data/News Articles/business/019.txt       business  019.txt   \n",
       "2          data/News Articles/business/260.txt       business  260.txt   \n",
       "3          data/News Articles/business/007.txt       business  007.txt   \n",
       "4          data/News Articles/business/042.txt       business  042.txt   \n",
       "...                                        ...            ...      ...   \n",
       "2220  data/News Articles/entertainment/335.txt  entertainment  335.txt   \n",
       "2221  data/News Articles/entertainment/006.txt  entertainment  006.txt   \n",
       "2222  data/News Articles/entertainment/187.txt  entertainment  187.txt   \n",
       "2223  data/News Articles/entertainment/034.txt  entertainment  034.txt   \n",
       "2224  data/News Articles/entertainment/228.txt  entertainment  228.txt   \n",
       "\n",
       "                                                   text  topicLabel  \n",
       "0     Italy to get economic action plan\\n\\nItalian P...           0  \n",
       "1     India widens access to telecoms\\n\\nIndia has r...           0  \n",
       "2     Asia shares defy post-quake gloom\\n\\nIndonesia...           0  \n",
       "3     Jobs growth still slow in the US\\n\\nThe US cre...           0  \n",
       "4     UK Coal plunges into deeper loss\\n\\nShares in ...           0  \n",
       "...                                                 ...         ...  \n",
       "2220  De Niro film leads US box office\\n\\nFilm star ...           1  \n",
       "2221  Bennett play takes theatre prizes\\n\\nThe Histo...           1  \n",
       "2222  Double eviction from Big Brother\\n\\nModel Capr...           1  \n",
       "2223  Vera Drake scoops film award\\n\\nOscar hopefuls...           1  \n",
       "2224  Joy Division story to become film\\n\\nThe life ...           1  \n",
       "\n",
       "[2225 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check out our corpus DataFrame\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4b125-00f5-41fb-917e-f29d153c6b43",
   "metadata": {},
   "source": [
    "### Shuffle, Stratify, Test, Train, Split \n",
    "\n",
    "So, unlike Version 1.0, here we'll be following more well established procedure and splitting out our test dataset early. We'll also be doing some stratification, to ensure our test and test datasets are split proportionally.\n",
    "\n",
    "This isn't going to help our ML model that much if our data topics have massively different populations, but I know already they are just about okay. We can shelve the outlier cases for another project. \n",
    "\n",
    "(I want to be a bit carful of my wording here. I have read that if the populations counts are very different, your ML of choice will struggle and you may need to do some sort of normalization process. Google imbalanced classes. This is a different problem to feature scaling.) See also [this link](https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586aa3e4-819d-4900-91ac-a6f39dab1352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14835d1a-a34c-4a84-8ea9-e75435ac0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(corpus, corpus['topicLabel']):\n",
    "    strat_train_set = corpus.loc[train_index]\n",
    "    strat_test_set = corpus.loc[test_index]\n",
    "# We're splitting 10 times (actually the default value for n_spilts) \n",
    "# to reshuffle the data 10 times. Somewhat overkill, and might quickly\n",
    "# blow up for bigger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46dc41be-28cb-4db9-be8d-c0453fb53ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            0.229775\n",
       "business         0.229213\n",
       "politics         0.187079\n",
       "tech             0.180337\n",
       "entertainment    0.173596\n",
       "Name: topic, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set['topic'].value_counts()/len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a7c544b-8c6d-4f73-8538-d6449bfe2cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            0.229663\n",
       "business         0.229213\n",
       "politics         0.187416\n",
       "tech             0.180225\n",
       "entertainment    0.173483\n",
       "Name: topic, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['topic'].value_counts()/len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b7908b-8378-4055-9835-1b3da8b1a584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>topic</th>\n",
       "      <th>fileName</th>\n",
       "      <th>text</th>\n",
       "      <th>topicLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>data/News Articles/business/391.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>391.txt</td>\n",
       "      <td>Yukos heading back to US courts\\n\\nRussian oil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>data/News Articles/tech/022.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>022.txt</td>\n",
       "      <td>Sun offers processing by the hour\\n\\nSun Micro...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>data/News Articles/business/072.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>072.txt</td>\n",
       "      <td>S Korean consumers spending again\\n\\nSouth Kor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>data/News Articles/business/408.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>408.txt</td>\n",
       "      <td>South African car demand surges\\n\\nCar manufac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>data/News Articles/tech/117.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>117.txt</td>\n",
       "      <td>Joke e-mail virus tricks users\\n\\nA virus that...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>data/News Articles/business/270.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>270.txt</td>\n",
       "      <td>Oil prices reach three-month low\\n\\nOil prices...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>data/News Articles/tech/310.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>310.txt</td>\n",
       "      <td>Latest Opera browser gets vocal\\n\\nNet browser...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>data/News Articles/business/179.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>179.txt</td>\n",
       "      <td>Irish duo could block Man Utd bid\\n\\nIrishmen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>data/News Articles/sport/396.txt</td>\n",
       "      <td>sport</td>\n",
       "      <td>396.txt</td>\n",
       "      <td>Wales silent on Grand Slam talk\\n\\nRhys Willia...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>data/News Articles/politics/042.txt</td>\n",
       "      <td>politics</td>\n",
       "      <td>042.txt</td>\n",
       "      <td>Strike threat over pension plans\\n\\nMillions o...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filePath     topic fileName  \\\n",
       "56    data/News Articles/business/391.txt  business  391.txt   \n",
       "558       data/News Articles/tech/022.txt      tech  022.txt   \n",
       "42    data/News Articles/business/072.txt  business  072.txt   \n",
       "485   data/News Articles/business/408.txt  business  408.txt   \n",
       "879       data/News Articles/tech/117.txt      tech  117.txt   \n",
       "...                                   ...       ...      ...   \n",
       "253   data/News Articles/business/270.txt  business  270.txt   \n",
       "778       data/News Articles/tech/310.txt      tech  310.txt   \n",
       "174   data/News Articles/business/179.txt  business  179.txt   \n",
       "1358     data/News Articles/sport/396.txt     sport  396.txt   \n",
       "915   data/News Articles/politics/042.txt  politics  042.txt   \n",
       "\n",
       "                                                   text  topicLabel  \n",
       "56    Yukos heading back to US courts\\n\\nRussian oil...           0  \n",
       "558   Sun offers processing by the hour\\n\\nSun Micro...           4  \n",
       "42    S Korean consumers spending again\\n\\nSouth Kor...           0  \n",
       "485   South African car demand surges\\n\\nCar manufac...           0  \n",
       "879   Joke e-mail virus tricks users\\n\\nA virus that...           4  \n",
       "...                                                 ...         ...  \n",
       "253   Oil prices reach three-month low\\n\\nOil prices...           0  \n",
       "778   Latest Opera browser gets vocal\\n\\nNet browser...           4  \n",
       "174   Irish duo could block Man Utd bid\\n\\nIrishmen ...           0  \n",
       "1358  Wales silent on Grand Slam talk\\n\\nRhys Willia...           3  \n",
       "915   Strike threat over pension plans\\n\\nMillions o...           2  \n",
       "\n",
       "[1780 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410666b3-2ed7-489d-83bd-8a181145a760",
   "metadata": {},
   "source": [
    "### Transformation Pipeline\n",
    "\n",
    "It is more consistent in bigger projects to ensure your transformations occur within a sklearn pipeline. This allows for more interoperabiltiy with sklearn tools, and allows for a more logical understanding of the steps taken. Or at least I think so. \n",
    "\n",
    "Anyway, here I am going ahead and creating a pipeline which carries out the **TfidfVectorization**, giving myself the flexibility _later_ to change properties such as:\n",
    "* stop words\n",
    "* ngram range\n",
    "* other stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d805edf-e93e-4a80-a105-919ab56d67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e999d5d-382e-40bb-9f99-cf2f831a97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "textPreprocess_pipeline = Pipeline([\n",
    "     (\"tfidfVec\", TfidfVectorizer(stop_words = 'english',ngram_range = (1,1)))\n",
    "]) # for now I'm setting some keyword parameters to be explicit \n",
    "\n",
    "# These lines below are not used, but are here just to demonstrate how a parameter matrix is set up ...I think.\n",
    "# Will need to be deleted/moved to the correct section at a later iteration. \n",
    "tf_params = {\n",
    "    'tfidfVec__ngram_range':((1,1),(1,2)),\n",
    "    'tfidfVec__stop_words':(None,'english')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07519d11-7bd6-40ed-89e3-5458ecfb123c",
   "metadata": {},
   "source": [
    "### An Open Question\n",
    "\n",
    "Should this **TfidfVectorization** step (and any later alterations to the text) be part of a discreet pre-processing step that is outside of GridSearch/Cross_val/RandomSearch? Or should it be part of one pipeline, including the model? I honestly don't know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37afe64f-8a18-4a41-aa20-49d69e095cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try out this pipeline first, before we add more to it\n",
    "\n",
    "tfidfTrain = textPreprocess_pipeline.fit_transform(strat_train_set['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5155bec5-9594-4d0f-a99e-9933a46340ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting components from a pipeline needs a little more unpacking, but it is possible\n",
    "tfidf_features = textPreprocess_pipeline.named_steps['tfidfVec'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cf4434b-a610-493f-a907-84ade1f4f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>000bn</th>\n",
       "      <th>000m</th>\n",
       "      <th>000s</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>001and</th>\n",
       "      <th>001st</th>\n",
       "      <th>...</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zornotza</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zubair</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>zvyagintsev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 26460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00       000  0001  000bn  000m  000s  000th  001  001and  001st  ...  \\\n",
       "0     0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "1     0.0  0.023236   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "2     0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "3     0.0  0.042693   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "4     0.0  0.023043   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "...   ...       ...   ...    ...   ...   ...    ...  ...     ...    ...  ...   \n",
       "1775  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "1776  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "1777  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "1778  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "1779  0.0  0.021012   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
       "\n",
       "      zooms  zooropa  zornotza  zorro  zubair  zuluaga  zurich  zutons  \\\n",
       "0       0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "1       0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "2       0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "3       0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "4       0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "...     ...      ...       ...    ...     ...      ...     ...     ...   \n",
       "1775    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "1776    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "1777    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "1778    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "1779    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
       "\n",
       "      zvonareva  zvyagintsev  \n",
       "0           0.0          0.0  \n",
       "1           0.0          0.0  \n",
       "2           0.0          0.0  \n",
       "3           0.0          0.0  \n",
       "4           0.0          0.0  \n",
       "...         ...          ...  \n",
       "1775        0.0          0.0  \n",
       "1776        0.0          0.0  \n",
       "1777        0.0          0.0  \n",
       "1778        0.0          0.0  \n",
       "1779        0.0          0.0  \n",
       "\n",
       "[1780 rows x 26460 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO DO - Beautify output for demonstration\n",
    "pd.DataFrame(data = tfidfTrain.toarray(),columns = tfidf_features)\n",
    "#textPreprocess_pipeline.named_steps['tfidfVec'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e48217-c7fc-48f5-966b-57fb8536b8dd",
   "metadata": {},
   "source": [
    "### The Full Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe793a-def4-448c-91c8-8605d030f873",
   "metadata": {},
   "source": [
    "We now bring in a model selection, which for a simple first-pass classification we will use a linear model such as SVC (supprt vector... classifier?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbe5bd3b-3895-47ba-a14c-c9b20287e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d36d11f-f2a3-4ce9-9f0b-04ad434c50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our full pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    (\"tfidfVec\", TfidfVectorizer(stop_words = 'english',ngram_range = (1,1))),\n",
    "    ('svm',SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb44bf2-c4cc-40e7-bbe6-3d8e7df328cf",
   "metadata": {},
   "source": [
    "### Cross Val Score \n",
    "\n",
    "The cross val method is a common way to 'cheat' with your data and create sub-sets of test-train datasets using slices of only the training data. While this is done at the expense of computing power & time, it can ensure your model is overall more reliable. (I need to work on this wording, it's not the most convincing. Why is it more reliable? Less overfitting? More tests = more trustworthiness in find outputs?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04bfddbf-5bd1-4ed9-b120-33e6712122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42f97b37-2be3-4b35-99f1-11feba89169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfSVM_cvs = cross_val_score(model_pipeline,\n",
    "                            X = strat_train_set['text'], \n",
    "                            y = strat_train_set['topicLabel'],\n",
    "                            cv = 5, # i.e. 3 cross val splits\n",
    "                            scoring = 'accuracy' # default for SVC\n",
    "                           )\n",
    "# Warning -- Accuracy is not a typically a good metric for classification\n",
    "# will be writing more on this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6af2742-0296-4663-bf17-16b81a4aea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 5 cross-val splits, the model accuracy outputs:\n",
      "\n",
      "[0.98314607 0.96910112 0.9747191  0.96910112 0.98314607]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfSVM_cvs\n",
    "print(\"For {} cross-val splits, the model accuracy outputs:\\n\".format(len(tfSVM_cvs)))\n",
    "print(\"{}\\n\".format(tfSVM_cvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60774c75-c49d-46f3-9c99-cf5eb45d1b60",
   "metadata": {},
   "source": [
    "### Classification Accuracy\n",
    "\n",
    "When it comes to classification problems, \"accuracy\" is often not the most reliable metric to judge your model. Admittedly, the raw outputs above are pretty good, and high enough be better than a \"dumb\" estimator that simply just guesses the topic. \n",
    "\n",
    "Instead, we'll look at measures such as the:\n",
    "* **precision**\n",
    "* **recall** \n",
    "* and the **confusion matrix**\n",
    "\n",
    "These can be created using **cross_val_predict**. (I need a better explination here as to why we use cross_val_predict. The accuracy gives us an idea of how well the model does with respect to the predicted label compared to the actual label. This seems fine, but it doesn't give us a view on how many of those cases are false positives or false negatives. Of course, if the accuracy is 100%, this wouldn't be an issue really. However, knowing the rates of false +ives and false -ives gives us a better insight into how the model works, and lets us make better choices in tuning our model for the desired output based on our use-case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e334b6b1-1985-4ce9-a9bb-8020f68f3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19a67c30-94f0-428b-be1c-4dcdd9ac7005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97979798, 0.96795953, 0.96795953])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfSVM_cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f8cca-cc4f-4769-98ec-0963786c9b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
