{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b5bc59-2d54-4124-b6d5-1f200d243868",
   "metadata": {},
   "source": [
    "# Supervised Topic Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643f623-96e4-45ab-8cdd-24ae06f35a38",
   "metadata": {},
   "source": [
    "## Version 2.0\n",
    "\n",
    "Welcome to my Supervised Toppic Classification Learning Experiment, using the BBC News Kaggle dataset. This series of Notebooks is to serve myself, and potentially others, as I learn bits of NLP relating to topic classification. Each notebook is split into versions to capture my design and learning progress, and to demonstrate in situ the mistakes and poor judgement calls I make. \n",
    "\n",
    "As the end of each iteration, I will leave some notes to discuss how I will (hopefully) improve my work in future versions. \n",
    "\n",
    "See data source here https://www.kaggle.com/pariza/bbc-news-summary \n",
    "\n",
    "Last accessed on 2022-Mar-12.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d815c9ea-865c-4090-95e6-83227ce9e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some standard dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe09a7d-ede8-4e23-b596-7a3964049023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're cloning the gitlab repo, this cell wont work for you unless you save the data until a similar directory.\n",
    "# Data is available in Notebook header text. \n",
    "\n",
    "import glob\n",
    "files = glob.glob('data/News Articles/**/*.txt',\n",
    "                  recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc4317-7360-404e-8835-d2ccd2c1b8b1",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The data repo contains **five** folders, {business, entertainment, politics, sport, tech} each containing a `.txt` file corresponding to one Article. \n",
    "\n",
    "I know already, because I looked at *some* of them, that these files are pretty clean. There is no header/footer text to remove or other structural features, but there are a lot of symbols for currency (£, $, Eur), percentages (%) etc. So these will need to be handled. \n",
    "\n",
    "**Things to consider**\n",
    "* The variation in article length\n",
    "* The number of articles per 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2b0bc62-0475-4894-b90e-7a91c7f1b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create data 'labels' but using the name of the folder the Article is saved in\n",
    "labels,fileNames = zip(*[s.split('/')[2:] for s in files])\n",
    "# Then make a pandas.DataFrame to store the data\n",
    "corpus = pd.DataFrame(data = {\n",
    "    \"filePath\":files, \"topic\":labels, \"fileName\":fileNames})\n",
    "\n",
    "# Open and read each file, save to list, and append to the corpus pd.DataFrame\n",
    "# \"unicode_escape\" deals with the symbols in the text\n",
    "text = []\n",
    "for name in files:\n",
    "    with open(name,\"r\",encoding=\"unicode_escape\") as f:\n",
    "        text.append(f.read())\n",
    "corpus['text'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693110b8-6885-45b0-a1bf-e4be1aa77feb",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "This task is strictly multi-Class classification. See https://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "We want our articles to have one label (target). They are either business, or sport etc. but cannot be both. Having multiple possible labels would be a multi-LABEL classification problem. \n",
    "\n",
    "The type of problem determines how we need to encode our labels. \n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "\n",
    "and https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "We will be doing Label Encoding, as we want to turn our string labels into numerical values which go from 0 to 1-n, where n = number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de00da7-747b-4f98-85c9-be15f65d1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Encoded labels\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(corpus['topic'].unique())\n",
    "corpus['topicLabel'] = le.transform(corpus['topic'])\n",
    "# Is this overkill? Maybe. It it makes me happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "382e4a30-2cbf-4960-ad0a-c29965abf87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>topic</th>\n",
       "      <th>fileName</th>\n",
       "      <th>text</th>\n",
       "      <th>topicLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/News Articles/business/052.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>052.txt</td>\n",
       "      <td>Italy to get economic action plan\\n\\nItalian P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/News Articles/business/019.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>019.txt</td>\n",
       "      <td>India widens access to telecoms\\n\\nIndia has r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/News Articles/business/260.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>260.txt</td>\n",
       "      <td>Asia shares defy post-quake gloom\\n\\nIndonesia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/News Articles/business/007.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>007.txt</td>\n",
       "      <td>Jobs growth still slow in the US\\n\\nThe US cre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/News Articles/business/042.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>042.txt</td>\n",
       "      <td>UK Coal plunges into deeper loss\\n\\nShares in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>data/News Articles/entertainment/335.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>335.txt</td>\n",
       "      <td>De Niro film leads US box office\\n\\nFilm star ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>data/News Articles/entertainment/006.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>006.txt</td>\n",
       "      <td>Bennett play takes theatre prizes\\n\\nThe Histo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>data/News Articles/entertainment/187.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>187.txt</td>\n",
       "      <td>Double eviction from Big Brother\\n\\nModel Capr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>data/News Articles/entertainment/034.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>034.txt</td>\n",
       "      <td>Vera Drake scoops film award\\n\\nOscar hopefuls...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>data/News Articles/entertainment/228.txt</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>228.txt</td>\n",
       "      <td>Joy Division story to become film\\n\\nThe life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      filePath          topic fileName  \\\n",
       "0          data/News Articles/business/052.txt       business  052.txt   \n",
       "1          data/News Articles/business/019.txt       business  019.txt   \n",
       "2          data/News Articles/business/260.txt       business  260.txt   \n",
       "3          data/News Articles/business/007.txt       business  007.txt   \n",
       "4          data/News Articles/business/042.txt       business  042.txt   \n",
       "...                                        ...            ...      ...   \n",
       "2220  data/News Articles/entertainment/335.txt  entertainment  335.txt   \n",
       "2221  data/News Articles/entertainment/006.txt  entertainment  006.txt   \n",
       "2222  data/News Articles/entertainment/187.txt  entertainment  187.txt   \n",
       "2223  data/News Articles/entertainment/034.txt  entertainment  034.txt   \n",
       "2224  data/News Articles/entertainment/228.txt  entertainment  228.txt   \n",
       "\n",
       "                                                   text  topicLabel  \n",
       "0     Italy to get economic action plan\\n\\nItalian P...           0  \n",
       "1     India widens access to telecoms\\n\\nIndia has r...           0  \n",
       "2     Asia shares defy post-quake gloom\\n\\nIndonesia...           0  \n",
       "3     Jobs growth still slow in the US\\n\\nThe US cre...           0  \n",
       "4     UK Coal plunges into deeper loss\\n\\nShares in ...           0  \n",
       "...                                                 ...         ...  \n",
       "2220  De Niro film leads US box office\\n\\nFilm star ...           1  \n",
       "2221  Bennett play takes theatre prizes\\n\\nThe Histo...           1  \n",
       "2222  Double eviction from Big Brother\\n\\nModel Capr...           1  \n",
       "2223  Vera Drake scoops film award\\n\\nOscar hopefuls...           1  \n",
       "2224  Joy Division story to become film\\n\\nThe life ...           1  \n",
       "\n",
       "[2225 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check out our corpus DataFrame\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4b125-00f5-41fb-917e-f29d153c6b43",
   "metadata": {},
   "source": [
    "### Shuffle, Stratify, Test, Train, Split \n",
    "\n",
    "So, unlike Version 1.0, here we'll be following more well established procedure and splitting out our test dataset early. We'll also be doing some stratification, to ensure our test and test datasets are split proportionally.\n",
    "\n",
    "This isn't going to help that much if our datasets are massively different, but I know already they are just about okay. We can shelve the outlier cases for another project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "586aa3e4-819d-4900-91ac-a6f39dab1352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14835d1a-a34c-4a84-8ea9-e75435ac0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(corpus, corpus['topicLabel']):\n",
    "    strat_train_set = corpus.loc[train_index]\n",
    "    strat_test_set = corpus.loc[test_index]\n",
    "# We're splitting 10 times (actually the default) just \n",
    "# to reshuffle 10 times. Somewhat overkill, and might quickly\n",
    "# blow up for bigger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46dc41be-28cb-4db9-be8d-c0453fb53ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            0.229775\n",
       "business         0.229213\n",
       "politics         0.187079\n",
       "tech             0.180337\n",
       "entertainment    0.173596\n",
       "Name: topic, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set['topic'].value_counts()/len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a7c544b-8c6d-4f73-8538-d6449bfe2cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            0.229663\n",
       "business         0.229213\n",
       "politics         0.187416\n",
       "tech             0.180225\n",
       "entertainment    0.173483\n",
       "Name: topic, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['topic'].value_counts()/len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b719f-25a9-4d74-850e-6ac7eb75e96b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
